# ‚õΩ Carburant ETL ‚Äì Pipeline de traitement des prix des carburants en France

Un pipeline **ETL local modulaire**, d√©velopp√© en Python, orchestr√© avec **Airflow** et stock√© dans **PostgreSQL**, pour extraire, transformer et charger les donn√©es de prix de carburants en France publi√©es par le gouvernement (open data).

---

## üöÄ Objectifs

- Extraire les donn√©es XML compress√©es depuis l‚ÄôAPI publique [roulez-eco.fr](https://donnees.roulez-eco.fr/opendata/instantane)
- D√©compresser, parser et transformer les donn√©es avec pandas
- Charger dans une base PostgreSQL locale
- Orchestrer le tout avec Apache Airflow (Docker)
- Centraliser les param√®tres dans un fichier `.env`

---

## üß∞ Stack technique

- Python 3.12
- Apache Airflow 2.9 (via Docker)
- PostgreSQL 15
- pandas
- psycopg2
- dotenv
- Docker / docker-compose

---

## ‚öôÔ∏è Installation et lancement

### 1. Cloner le d√©p√¥t

```bash
git clone https://github.com/<TON-UTILISATEUR>/carburant_etl.git
cd carburant_etl
```

### 2. Cr√©er les dossiers de travail

```bash
mkdir -p airflow/data airflow/logs airflow/plugins
```

### 3. Cr√©er le fichier `.env`

```env
DB_HOST=postgres
DB_PORT=5432
DB_NAME=airflow
DB_USER=airflow
DB_PASSWORD=airflow

RAW_DATA_PATH=/opt/airflow/data/PrixCarburants_instantane.xml
CLEAN_DATA_PATH=/opt/airflow/data/clean.csv
DATA_DIR=/opt/airflow/data
```

### 4. D√©marrer les services avec Docker

```bash
docker-compose up --build
```

### 5. Acc√©der √† Airflow

- URL : [http://localhost:8080](http://localhost:8080)
- Identifiant : `admin`
- Mot de passe : `admin`

---

## üìä Donn√©es utilis√©es

- Source : [data.gouv.fr ‚Äì prix des carburants](https://www.data.gouv.fr/fr/datasets/prix-des-carburants-en-france-flux-instantane)
- Format : XML compress√© `.zip`
- Champs : nom station, ville, code postal, carburant, prix, date de mise √† jour

---

## üß™ Qualit√© des donn√©es

- Suppression des lignes avec `prix <= 0` ou valeurs manquantes
- Parsing strict des dates et types num√©riques
- T√¢che `cleanup_files` int√©gr√©e au DAG

---

## üîÑ DAG Airflow

Le pipeline Airflow est compos√© de 4 t√¢ches :

1. `extract_data` ‚Äì T√©l√©charge et d√©compresse les donn√©es XML
2. `transform_data` ‚Äì Nettoie et transforme les donn√©es
3. `load_data` ‚Äì Charge les donn√©es dans PostgreSQL
4. `cleanup_files` ‚Äì Supprime les fichiers temporaires

Planifi√© toutes les **10 minutes** (`*/10 * * * *`).

---

## ‚úÖ Am√©liorations possibles

- Ajouter des tests unitaires (avec pytest)
- Visualiser les donn√©es avec Streamlit ou Metabase
- Sauvegarder en Parquet pour simuler un datalake
- API FastAPI pour interroger les donn√©es

---

## üë®‚Äçüíª Auteur

**Manssour Ghorbel**  
Projet personnel de formation en data engineering  
üì´ [LinkedIn](https://www.linkedin.com/in/ghorbelmanssour/) *(√† adapter si besoin)*

---

## üìÑ Licence

Ce projet est sous licence MIT ‚Äì libre d‚Äôutilisation √† des fins p√©dagogiques.
